\chapter{Conclusion}
\label{ch:con}

In this thesis we have discussed the impact of LHC measurements upon the extraction of parton distribution functions, and methods for the inclusion of such data into parton fits. With the LHC due to begin collisions at centre of mass energies of $13$ TeV in 2015, the need for precise determinations of proton structure is as great as ever. Here we have described the efforts undertaken to provide the particle physics community with PDFs extracted via a methodologically sound procedure to an extensive experimental dataset including measurements from the first data runs of the LHC.

A number of tools have been developed that enable the study of a large collider dataset in the context of the NNPDF procedure. The Bayesian reweighting procedure, first implemented in a PDF determination by the NNPDF collaboration, allows for a rapid assessment of data impact. Bayesian reweighting also provides a deep check of the consistency of the NNPDF methodology, as a verification of the statistical behaviour of the Monte Carlo PDF ensemble. The method is however unsuitable for the inclusion of a large dataset, due to the need for an unpractically large prior Monte Carlo distribution. Therefore in order to enable the inclusion of a large LHC dataset in an NNPDF fit, further tools are required.

The {\tt FK} method was introduced as a method by which the PDF evolution may be combined with the theoretical predictions for experimental observables in a process independent way. The resulting matrices, or {\tt FK} tables provide an extremely efficient method of computing theoretical predictions based upon a varying input PDF set, reducing the task to a simple scalar product which can be efficiently optimised. This method enabled the inclusion of LHC data into a full NNPDF fit for the first time, without having to compromise on the accuracy of the calculation. While fast, the {\tt FK} procedure is rather specialised to the task of PDF fitting as it only permits for the variation of input PDF set. For wider studies of the dependence of theoretical predictions upon parameters such as the strong coupling or factorisation and renormalisation scales, more flexible approaches as implemented in packages such as {\tt FastNLO} or {\tt APPLgrid} are more relevant.

A package for the interfacing of automated NLO calculational tools to such fast interpolating codes has been developed. The {\tt MCgrid} package allows for the use of Monte Carlo event generators such as {\tt SHERPA} along with a suitable one-loop generator, for the efficient variation of QCD parameters in theoretical predictions. Such an interface opens up the possibility of using such codes in applications such as $\alpha_S$ determinations or PDF fits, alongside making PDF and scale variation more accessible in computationally challenging processes. The interface between event generators and interpolating tools remains however at the level of fixed order perturbation theory. In principle an extension for the fast computation of observables with parton shower effects included would be particularly desirable, making this an important avenue for future research.

These tools have been applied to the study of LHC measurements and their impact upon PDF distributions. To date two NNPDF results including LHC constraints have been published. The NNPDF2.2 determination included a set of $W$ boson production asymmetry measurements at both the LHC and the Tevatron, by the method of Bayesian reweighting. In such a way the reweighting method was extensively validated, and new constraints were placed upon PDFs from LHC data for the first time. As the dataset available from the LHC expanded, the need for a comprehensive refit including all appropriate measurements increased. The resulting PDF set, NNPDF2.3, utilised the {\tt FK} procedure for all of the included processes and so was able to include all available LHC measurements of interest to PDF fits at the time. The NNPDF2.3 set provided a precise determination ideally suited for further applications at the LHC.

Following the NNPDF2.3 set, the development of the {\tt nnpdf++} project has allowed for a greater scope in investigating methodological elements, permitting a large scale re-evaluation of the procedure used in the NNPDF2.3 family of fits. The closure testing procedure introduced in Chapter~\ref{ch:LHClight} now forming the basis for the development in methodology post-NNPDF2.3. Insights provided by a detailed study of the NNPDF procedure when applied to closure tests have informed a number of new approaches in minimisation and stopping for the next global PDF set produced by the collaboration. The next release, NNPDF3.0, being validated using the closure testing procedure and including an expanded LHC dataset will provide the most precise and methodologically sound determination of parton distribution functions.
